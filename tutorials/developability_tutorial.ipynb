{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "developability.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Our first neural network\n",
        "\n",
        "Today we will try to write a neural network that will predict antibody developability\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We will use data from [TDC](https://tdcommons.ai/single_pred_tasks/develop/), a collection of biochemical datasets for deep learning"
      ],
      "metadata": {
        "id": "3OQQ0Kkycfk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyTDC\n",
        "!pip install plotly\n",
        "!pip install fair-esm\n",
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "izUKywwWxGXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import List\n",
        "import re\n",
        "import esm\n",
        "from pytorch_lightning import Trainer, LightningModule\n",
        "%load_ext tensorboard\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "VJQJnJuwd9aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tdc.single_pred import Develop\n",
        "data = Develop(name = 'SAbDab_Chen')\n",
        "split = data.get_split()"
      ],
      "metadata": {
        "id": "m1v77bAVi1Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_chains(old_input:str) -> list:\n",
        "  translated_input = re.sub(\"[\\[\\]\\s']\", \"\", old_input)\n",
        "  return translated_input.split(\",\")"
      ],
      "metadata": {
        "id": "W9cltJE3jJGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [\"train\", \"valid\", \"test\"]:\n",
        "  split[i][\"Antibody\"] = split[i][\"Antibody\"].apply(split_chains)\n",
        "  split[i][\"Antibody\"] = split[i][\"Antibody\"].apply(lambda x: x[0] + x[1])\n",
        "train = split[\"train\"]\n",
        "valid = split[\"valid\"]\n",
        "test = split[\"test\"]"
      ],
      "metadata": {
        "id": "H63oLe3Zluur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Featurisation\n",
        "\n",
        "First we would need to write a function that takes our aminoacid sequence `EVQLQQSGAEVVRSGAS` and converts it to a tensor of numbers.\n",
        "\n",
        "The rough steps for that would be:\n",
        "\n",
        "1. Identify our alphabet (all available amino acids)\n",
        "1. Assign a number to each amino acid\n",
        "1. \"Translate\" our sequences into numbers\n",
        "1. _Pad_ sequences to give them identical lengths\n",
        "\n",
        "Then we would want to apply it to both sequences in our input "
      ],
      "metadata": {
        "id": "EGdO_KEKdNdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = set()\n",
        "for sequence in train[\"Antibody\"]:\n",
        "  for letter in sequence:\n",
        "    alphabet.add(letter)\n",
        "alphabet_dict = {i:idx for idx, i in enumerate(sorted(alphabet))}\n",
        "alphabet_dict[\"$\"] = len(alphabet)\n",
        "alphabet_dict"
      ],
      "metadata": {
        "id": "j9XVY0Xestsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "esm_model, alphabet = esm.pretrained.esm1_t6_43M_UR50S()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "esm = esm_model.eval()  # disables dropout for deterministic results\n",
        "esm_model = esm_model.cuda()"
      ],
      "metadata": {
        "id": "bqREl2W2uGIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sequence(sequence:str, max_length:int=281) -> List[int]:\n",
        "  result = []\n",
        "  n = len(sequence)\n",
        "  pad_size = max_length - n\n",
        "  for letter in sequence:\n",
        "    result.append(alphabet_dict.get(letter, alphabet_dict[\"$\"]))\n",
        "  if n > max_length:\n",
        "    result = result[:max_length]\n",
        "  else:\n",
        "    result += [20] * pad_size\n",
        "  return result\n",
        "\n",
        "def get_esm_gpu(seqs:list, esm_model, batch_size=6):\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(seqs)\n",
        "  dl = torch.utils.data.DataLoader(batch_tokens, batch_size=batch_size)\n",
        "  embeddings = []\n",
        "  with torch.no_grad():\n",
        "    for batch in dl:\n",
        "      batch = batch.cuda()\n",
        "      results = esm_model(batch, repr_layers=[6], return_contacts=True)\n",
        "      token_representations = results[\"representations\"][6]\n",
        "      embeddings.append(token_representations.mean(1).cpu())\n",
        "  return torch.cat(embeddings)"
      ],
      "metadata": {
        "id": "PtL8Hk_a5gRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESM use example\n",
        "seqs_test = [(\"seq1\", \"KKKKKKRKRKRKRK\"), (\"seq2\", \"RRRRRRR\"), (\"seq2\", \"VKRKRKRKVKVKVKMKMKMK\")]\n",
        "seqs_embed = get_esm_gpu(seqs_test, esm_model)\n",
        "seqs_embed.size()"
      ],
      "metadata": {
        "id": "g85truXc7nF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def prepare_data(df:pd.DataFrame) -> torch.Tensor:\n",
        "  train_x = torch.tensor(df[\"Antibody\"].apply(translate_sequence))\n",
        "  train_y = torch.tensor(df[\"Y\"]).unsqueeze(-1)\n",
        "  return torch.hstack((train_x, train_y))\n",
        "\n",
        "def prepare_esm_data(df:pd.DataFrame) -> torch.Tensor:\n",
        "  \"\"\"Implement me please!\"\"\"\n",
        "  seq_pairs = []\n",
        "  for name, row in df.iterrows():\n",
        "    seq_pairs.append((row[\"Antibody_ID\"], row[\"Antibody\"]))\n",
        "  train_x = get_esm_gpu(seq_pairs, esm_model)\n",
        "  train_y = torch.tensor(df[\"Y\"]).unsqueeze(-1)\n",
        "  return torch.hstack((train_x, train_y))\n",
        "\n",
        "train_data = prepare_esm_data(train)\n",
        "val_data = prepare_esm_data(valid)\n",
        "test_data = prepare_esm_data(test)"
      ],
      "metadata": {
        "id": "wncJ2cNjuG8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.functional import accuracy\n",
        "\n",
        "class ESMModel(LightningModule):\n",
        "  def __init__(self, input_dim:int=768, hidden_dim:int=512, dropout:float=0.35, lr:float=0.001):\n",
        "    super().__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "    self.linear2 = torch.nn.Linear(hidden_dim, hidden_dim//2)\n",
        "    self.linear3 = torch.nn.Linear(hidden_dim//2, 1)\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "    self.lr = lr\n",
        "  \n",
        "  def forward(self, input:torch.Tensor) -> torch.Tensor:\n",
        "    x = self.linear1(input)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    return self.linear3(x)\n",
        "  \n",
        "  def shared_step(self, batch:torch.Tensor, step_type:str):\n",
        "    input = batch[:, :-1]\n",
        "    target = batch[:, -1]\n",
        "    pred = self.forward(input).squeeze(-1)\n",
        "    loss = F.binary_cross_entropy_with_logits(pred, target)\n",
        "    acc = accuracy(pred, target.long())\n",
        "    self.log(f\"{step_type}_loss\", loss)\n",
        "    self.log(f\"{step_type}_acc\", acc)\n",
        "    return dict(loss=loss, acc=acc)\n",
        "  \n",
        "  def training_step(self, batch:torch.Tensor) -> dict:\n",
        "    return self.shared_step(batch, \"train\")\n",
        "  \n",
        "  def validation_step(self, batch:torch.Tensor, batch_idx:int) -> dict:\n",
        "    return self.shared_step(batch, \"val\")\n",
        "  \n",
        "  def test_step(self, batch:torch.Tensor, batch_idx:int) -> dict:\n",
        "    return self.shared_step(batch, \"test\")\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optim = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
        "    scheduler = {\n",
        "            \"monitor\": \"val_loss\",\n",
        "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optim,\n",
        "                verbose=True,\n",
        "                factor=0.1,\n",
        "                patience=60,\n",
        "            ),\n",
        "        }\n",
        "    return [optim], [scheduler]\n"
      ],
      "metadata": {
        "id": "Cx47cLod7zfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "BATCH_SIZE=1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "model = ESMModel(hidden_dim=1024, lr=0.0001)\n",
        "trainer = Trainer(gpus=1, \n",
        "                  log_every_n_steps=10, \n",
        "                  max_epochs=1000, \n",
        "                  callbacks=[EarlyStopping(\"val_loss\", patience=200), \n",
        "                             ModelCheckpoint(monitor=\"val_loss\")], \n",
        "                  gradient_clip_val=50)\n",
        "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "trainer.test(model, test_loader)"
      ],
      "metadata": {
        "id": "lpSvh0hEG6MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir lightning_logs"
      ],
      "metadata": {
        "id": "IOdM7sZ_JZLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B7zMmpN4gLRS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}