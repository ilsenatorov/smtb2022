{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch tutorial\n",
        "\n",
        "In this tutorial I will try to guide you through how to use PyTorch - the library for deep learning that we will use!\n",
        "\n",
        "[PyTorch cheatsheet](https://pytorch.org/tutorials/beginner/ptcheat.html)\n",
        "\n",
        "<img src='https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.oiQt_8Md2ucueqYIJ30b9gHaHa%26pid%3DApi&f=1' width=200>"
      ],
      "metadata": {
        "id": "HKPZwlExtM1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scalars, vectors, matrices and tensors\n",
        "\n",
        "### Scalars\n",
        "\n",
        "$1.54$ - this is a __scalar__ (a singular number). If it has decimal points, it's called a _float_, if it doesn't - an _integer_.\n",
        "\n",
        "### Vectors\n",
        "\n",
        "$[1.54, -1.38, 0.12, 5.56]$ - this is a __vector__ (a row of numbers). Each entry in a vector is a scalar. Also can be called an _array_ or a _list_. Vectors are always one-dimensional.\n",
        "\n",
        "### Matrix \n",
        "\n",
        "$[[1.54, -1.38, 0.12, 5.56],$\n",
        "\n",
        "$[2.71, -2.23, 0.15, 4.56],$\n",
        "\n",
        "$[1.55, -2.87, 0.18, 3.56]]$\n",
        "\n",
        "This is a __matrix__ - 2-dimensional storage. Each row (and each column) in a matrix is a vector and each entry is a scalar.\n",
        "\n",
        "### Tensor\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSrj-eaRaN-2Qxsl-LSPzj7wgzvr4635kaR4XNnvXpvaekj06bkHDpfW5wl4fpJZ5fjMIA&usqp=CAU\" width=300>\n",
        "\n",
        "This is a 3d tensor. It is a collection of 2d matrices. Tensors can be of any number of dimensions.\n"
      ],
      "metadata": {
        "id": "dDA-loxPvL0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "KPvnDqnCw8DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.rand(2,3,5) # uniform distribution from 0 to 1\n",
        "print(t1)\n",
        "print(t1.size())\n",
        "t1 = t1.unsqueeze(-1)\n",
        "print(t1.size())\n",
        "t1 = t1.squeeze(-1)\n",
        "print(t1.size())\n",
        "t1 = t1.transpose(0,2)\n",
        "print(t1.size())\n",
        "print(t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X34MdXNYvKfZ",
        "outputId": "1084ef2f-42b7-49ff-a49e-e3b463f771c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.8740, 0.9985, 0.4113, 0.9648, 0.4899],\n",
            "         [0.5985, 0.2744, 0.1495, 0.1254, 0.8483],\n",
            "         [0.1827, 0.3457, 0.3793, 0.5476, 0.2959]],\n",
            "\n",
            "        [[0.4161, 0.6293, 0.5717, 0.7178, 0.4259],\n",
            "         [0.2044, 0.3851, 0.2665, 0.5535, 0.2831],\n",
            "         [0.4750, 0.7619, 0.5655, 0.5537, 0.6160]]])\n",
            "torch.Size([2, 3, 5])\n",
            "torch.Size([2, 3, 5, 1])\n",
            "torch.Size([2, 3, 5])\n",
            "torch.Size([5, 3, 2])\n",
            "tensor([[[0.8740, 0.4161],\n",
            "         [0.5985, 0.2044],\n",
            "         [0.1827, 0.4750]],\n",
            "\n",
            "        [[0.9985, 0.6293],\n",
            "         [0.2744, 0.3851],\n",
            "         [0.3457, 0.7619]],\n",
            "\n",
            "        [[0.4113, 0.5717],\n",
            "         [0.1495, 0.2665],\n",
            "         [0.3793, 0.5655]],\n",
            "\n",
            "        [[0.9648, 0.7178],\n",
            "         [0.1254, 0.5535],\n",
            "         [0.5476, 0.5537]],\n",
            "\n",
            "        [[0.4899, 0.4259],\n",
            "         [0.8483, 0.2831],\n",
            "         [0.2959, 0.6160]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshaping tensors\n",
        "\n",
        "Sometimes we need to change the dimensionality of a tensor, in order to perform a certain operation. This is called a _view_."
      ],
      "metadata": {
        "id": "Bw0rI95X0GRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.randint(0, 10, (3,2,4)) # (low, high, tuple with tensor sizes)\n",
        "print(t1)\n",
        "print(t1.view(6,-1))\n",
        "print(t1.view(-1)) # calculate last dim automatically"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkTW5nYW0TQh",
        "outputId": "ea496f59-5c91-448a-911f-b988d079772f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[4, 4, 7, 7],\n",
            "         [6, 0, 0, 6]],\n",
            "\n",
            "        [[8, 6, 3, 3],\n",
            "         [6, 5, 0, 7]],\n",
            "\n",
            "        [[2, 2, 6, 9],\n",
            "         [8, 4, 8, 5]]])\n",
            "tensor([[4, 4, 7, 7],\n",
            "        [6, 0, 0, 6],\n",
            "        [8, 6, 3, 3],\n",
            "        [6, 5, 0, 7],\n",
            "        [2, 2, 6, 9],\n",
            "        [8, 4, 8, 5]])\n",
            "tensor([4, 4, 7, 7, 6, 0, 0, 6, 8, 6, 3, 3, 6, 5, 0, 7, 2, 2, 6, 9, 8, 4, 8, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix multiplication\n",
        "\n",
        "In linear algebra, there is an operation called _matrix multiplication_.\n",
        "\n",
        "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.g63nN35FrxSHUNeNFym0SgHaF7%26pid%3DApi&f=1\" width=500>\n",
        "\n",
        "<img src=\"https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fi.stack.imgur.com%2FyxMKj.png&f=1&nofb=1\" width=500>\n",
        "\n",
        "We can model the process of going from one layer of neural network to another as matrix multiplication\n",
        "\n",
        "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Forgs.mines.edu%2Fdaa%2Fwp-content%2Fuploads%2Fsites%2F38%2F2019%2F08%2F1_Gh5PS4R_A5drl5ebd_gNrg%402x.jpg&f=1&nofb=1\" width=500>\n",
        "\n",
        "\n",
        "Matrix dimensions have to match in order to perform multiplication!"
      ],
      "metadata": {
        "id": "1HrAm7jiyCjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = torch.tensor([[3,4], [2,1]])\n",
        "m2 = torch.tensor([[1,5], [3,7]])\n",
        "res = m1 @ m2 # can also use torch.matmul(m1, m2)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOTdsD5Aw6Ux",
        "outputId": "37f104ab-dc21-4012-956c-b493bf7bfa9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[15, 43],\n",
            "        [ 5, 17]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = torch.rand(10,5)\n",
        "m2 = torch.rand(5,1)\n",
        "res = m1 @ m2\n",
        "print(res.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFvCMXxFCl-f",
        "outputId": "8696e858-26e1-4b42-a565-a0f0dfdf385f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batching\n",
        "\n",
        "Use of tensors allows us to go perform operations on many samples at a time\n",
        "\n",
        "For example, a single colored image of size 128x128 can be represented as a 3d tensor of shape (128, 128, 3).\n",
        "\n",
        "If we want to process 32 images in a single operation, we can stack them together in a tensor of size (32, 128, 128, 3)."
      ],
      "metadata": {
        "id": "j62lgznZy5_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = torch.rand(128, 128, 3)\n",
        "batch = torch.stack([torch.rand(128, 128, 3) for _ in range(32)])\n",
        "print(batch.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzcERgDjBzOl",
        "outputId": "d2aa2d68-3c61-4255-dd55-a51003557def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 128, 128, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operating on batches\n",
        "\n",
        "Performing mathematical operations on batches is identical to performing them on matrices. We have to make sure that the last dimension of m1 matches the first dimension of m2 though!\n",
        "\n",
        "Good: $(512, 128, 128, 3) \\times (3, 12)$\n",
        "\n",
        "Bad: $(128, 3, 128) \\times (3, 12)$ <- will give an error \n",
        "\n",
        "Solution: use `Tensor.transpose(dim1, dim2)` to fix that."
      ],
      "metadata": {
        "id": "uaNya8Rvz5IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch.size())\n",
        "w = torch.randn(3, 12)\n",
        "print(w.size())\n",
        "res = batch @ w\n",
        "print(res.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S4TBc3FycZY",
        "outputId": "721ff2da-73c6-42e9-b737-2c836318c8a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 128, 128, 3])\n",
            "torch.Size([3, 12])\n",
            "torch.Size([32, 128, 128, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering\n",
        "\n",
        "Is an art of converting real-life objects (photos, text, videos, proteins) into _features_ - numbers that our model can work with.\n",
        "\n",
        "For some objects it is easier (images are just tensors), for some - harder.\n",
        "\n",
        "### Tokenisation \n",
        "\n",
        "In order to convert text to numbers, we need to _tokenise_ it:\n",
        "\n",
        "<img src=\"https://freecontent.manning.com/wp-content/uploads/Chollet_DLfT_01.png\" width=500>\n",
        "\n",
        "### n-grams\n",
        "\n",
        "For protein sequences, we have a couple of options. We could use each individual aminoacid as a unique token (then we will get alphabet of size 21).\n",
        "\n",
        "We could also use each pair of aminoacids as a single token (how many tokens we will have then?). This would be called a bi-gram."
      ],
      "metadata": {
        "id": "w5yEe7cK2hui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully connected neural networks\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/2560px-Neural_network.svg.png\" width=500>\n",
        "\n",
        "__Input neurons__ are our input data - pixel values, tokens etc.\n",
        "\n",
        "__Weights__ are our trainable parameters - we assume that if we find the right weights (and the right model architecture) we can make correct predictions.\n",
        "\n",
        "__Hidden neurons__ are the states of our model - we assume that they can \"decide\" whether to push the data forward or not.\n",
        "\n",
        "__Activation functions__ decide whether hidden neurons will allow the data to propagate further.\n",
        "\n",
        "__Output layer__ is our prediction - for a classification problem it is the class probability, for a regression problem it's the predicted value.\n",
        "\n",
        "## Activation functions\n",
        "\n",
        "Once we calculated the input value for each neuron (which is the values of all neurons in previous layers multiplied by their respective weights), we apply an activation function to that value.\n",
        "\n",
        "This function \"decides\" whether the signal should be propagated further or not.\n",
        "\n",
        "Examples of common activation functions are:\n",
        "\n",
        "* Sigmoid - $\\frac{1}{1 + exp(-x)}$\n",
        "* ReLU - $max(0,x)$\n",
        "\n",
        "\n",
        "<img src=\"https://pytorch.org/docs/stable/_images/Sigmoid.png\" width=400>\n",
        "\n",
        "<img src=\"https://pytorch.org/docs/stable/_images/ReLU.png\" width=400>"
      ],
      "metadata": {
        "id": "IdtBaajD4SXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Neural network as a math problem\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/j2qa7.png\" width=500>\n",
        "\n",
        "\n",
        "This way we can formulate our output $\\hat{Y}$ as a mathematical equation:\n",
        "\n",
        "$\\hat{Y} = f_2(f_1(XW_1)W_2)W_3$\n",
        "\n",
        "$\\hat{y} = f_2(f_1(x\\times w_1) \\times w_2) \\times w_3$\n",
        "\n",
        "And then the loss can be calculated from that:\n",
        "\n",
        "$L_{cross-entropy} = l(\\hat{Y}, Y)$\n",
        "\n",
        "The cool thing about all of that - it is a defined mathematical function, which means we can take a _derivative_ of the whole thing with respect to the weights (using chain rule).\n",
        "\n",
        "If we can take a derivative, we can find the gradient. Then we can use the gradient to minimise the loss (again, w.r.t. weights of the model).\n",
        "\n",
        "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.lYpF8xJ3TiDoq461I0AcOQHaEn%26pid%3DApi&f=1\" width=400>\n",
        "\n",
        "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.VvLkkH3MOBHJIRgUMSWCVgHaFS%26pid%3DApi&f=1\" width=400>\n",
        "\n",
        "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.AC-5oNFKLqNdl7EoxwFJNQHaEc%26pid%3DApi&f=1\" width=400>"
      ],
      "metadata": {
        "id": "TbG9ey0cDWh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using that, we can train neural networks by:\n",
        "\n",
        "1. Acquiring the dataset\n",
        "1. Featurising the data\n",
        "1. Initialising the model with random weights\n",
        "1. Defining the loss function\n",
        "1. Making predictions on a batch of data points\n",
        "1. Calculating the loss\n",
        "1. Adjusting the weights according to the gradients\n",
        "1. Repeating until we succeed"
      ],
      "metadata": {
        "id": "dFUSITfZ81Qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU\n",
        "\n",
        "Calculations that are required to train the networks are performed much faster on GPU - part of the PC responsible for graphics.\n",
        "\n",
        "In order to move tensors and models between CPU and GPU, we have to use special commands."
      ],
      "metadata": {
        "id": "v9LJuH2R9QsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.rand(16,10)\n",
        "if torch.cuda.is_available():\n",
        "  t1 = t1.cuda()\n",
        "\n",
        "nn = torch.nn.Linear(10, 20)\n",
        "if torch.cuda.is_available():\n",
        "  nn = nn.cuda()\n",
        "\n",
        "res = nn.forward(t1)\n",
        "print(res.size())\n",
        "print(res.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6upmv8h80sX",
        "outputId": "0b90ea47-9085-4ea8-dc86-b17aa6e56d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 20])\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU errors\n",
        "\n",
        "Sometimes, if we perform an operation on different devices, we can get an error.\n"
      ],
      "metadata": {
        "id": "9nk4PJ2G-RYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.rand(16, 10)\n",
        "nn = torch.nn.Linear(10,20)\n",
        "if torch.cuda.is_available():\n",
        "  t1 = t1.cuda()\n",
        "nn.forward(t1) # This gives an error, since t1 is on GPU and nn is not"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "cyPsKBMh-OFo",
        "outputId": "53c2b909-8bc1-49e2-a8ee-ea50f877762a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-d92ef995c5e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This gives an error, since t1 is on GPU and nn is not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fixing\n",
        "\n",
        "To fix such errors, ensure that all tensors and modules are on the same device"
      ],
      "metadata": {
        "id": "0FUZant5A7Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.rand(10,20)\n",
        "t2 = torch.rand(30,40)\n",
        "(t1 @ t2.view(20, -1)).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK-BJ_Iq-jRW",
        "outputId": "e623e85b-ed18-4d5d-dcd5-1175a754e6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 60])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aBJA90mzcY9b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}